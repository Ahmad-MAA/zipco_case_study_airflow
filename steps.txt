create folder
add data set
create ipynb
choose kerel
open terminal
intialisise git ----- git intialisise
create repo online
git status
git add .
git status
git commit -m "firt commit"
git config --global user.email "h@gmail.com"
git config --global user.name "AhmadD"
clear
git status
copy code from repopase and run
git branch to see the master branch
git push -u origin master
open new terminal
run pip install pandas
or from the  python note book run !pip install pandas azure_storage_blob dotenv
!python -m pip install python-dotenv
#Importing Necessary libraries
# Data Extraction
# Data cleaning and transformation
# Handle missing values( filling missing numeric values with the mean or median)
# Handling missing values (filling missing string object values with 'unknown')
data['Date'] = pd.to_datetime(data['Date'])
#Create tables
# Save data to as csv files
# Clear cached variables (.env)
Loading the data to azure
create each py for the pipeline
create the DAG script
use wsl to run airflow
ensure python3 is installed
create a virtual environment
update git ignore to ignore vertual environment folder
update git to orging
zipcoenv\bin\activate
create a requirement.txt folder
run command pip install -r requirements.txt
install pacakeges individually
airflow init db
create user accout
create a file create_user.txt
run the create command and input password
go to airflow directory from linux using cd/home/airflow
run / spind the server aiflow with webserver 8080
run the airflow schedulere all on wsl 
http://localhost:8080 on web pacakeges
log in on webpage
open wsl >>> .env >> cd /home/ahmad/airflow
make a new directory for the dag folder( mkdir zipco_food_dag)
run ls 
run sudo nano airflow.cfg 
chage dag file name to the new directory dag directory....
Ctrl + X, Y to save and enter to close
cd zipco_food_dag
open a new terminal  copy---  2
wsl console
copy file to airflow directory
cp zipco_transaction.csv /home/ahmad/airflow/zipco_food_dag
source new_env/bin/activate
cd /home/ahmad/airflow
cp .env /home/ahmad/airflow/zipco_food_dag
sudo nano .env ------- to check
copy all pipeline files
Ctrl C to end terminal

run / spind the server aiflow with webserver 8080
run the airflow schedulere all on wsl  virtual environment
end airflow pipeline

remove the file 
rm Extraction.py and other parts
run sudo nano dag_script.py
copy from the py. file and paste the Ctrl + X Y enter(wsl venv)
and for other files
run ls
run find -name folder_name
run locate file_name
run locate sudo apt-get install mlocate